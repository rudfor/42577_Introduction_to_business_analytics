{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Business Analytics\n",
    "\n",
    "## Lecture 3 - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's time to learn about Classification models. This notebook will run you through the essential concepts, and give examples on how to run multiple Classification algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's do some imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The first model of the lecture relates with Logistic Regression. It is important that you understand the mechanics well, because their are applicable in other types of Classification models_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want you to understand the intuition behind the logit function, so let's work on the basics. Imagine you want to make a function that determines the probability of some event (let's call it x). The \"event\" x could simply mean that \"the input data corresponds to class 1\" (in which case, ~x means \"the input data corresponds to class 0\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to plot such a function, let's just create a vector with values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px=np.arange(0.0001, 1, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we determine the odds ratio function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=px/(1-px)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(y,px)\n",
    "plt.xlim([0,50])\n",
    "plt.xlabel(\"odds ratio\")\n",
    "plt.ylabel(\"p(x)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the class, this function form is not ideal at all. What happens if we apply the log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.log(px/(1-px))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be clear with in plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,px)\n",
    "#plt.xlim([0,50])\n",
    "plt.xlabel(\"logit\")\n",
    "plt.ylabel(\"p(x)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, now it seems much more balanced, doesn't it? \n",
    "\n",
    "Understanding this function is important:\n",
    "- What is the probability of x (p(x)) when the log odds ratio is 0? \n",
    "\n",
    "- What is the odds ratio itself? \n",
    "\n",
    "- What about the extremes (when is it 1 and when is it 0?)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start playing with data. First we need to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.read_csv(\"NYC_taxis_weather_2016_with_dummies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the dataframe yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the index is no longer the time, let's put it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, imagine that you are an NYC taxi fleet manager. At each 15 minutes, you goal is to make sure your company has enough cars for very big spikes in demand across the city (like above 90 percentile). If you detect some very big spike in a specific area, you coordinate with the cars in the neighbourhood to go there. \n",
    "\n",
    "For this exercise, let's assume that area 1 is the only truly important for you. Doing this manually would be very tiring (if at all possible), so you rely on your Data Science skills to get a model that does it for you:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At each 15 minutes time interval, predict whether the next time interval will have a demand spike (\"stress\").**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first find the actual value above which you call it a \"stress\" situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_threshold=np.percentile(f['pickups1'], 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many demand pickups exist above percentile 90? And above other percentiles (e.g. percentile 50)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a new column (or variable) that is True when it is a \"stress\" scenario, and False when it is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do what to inspect this new data that you created? (e.g. use describe(), hist(), etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create our model. The first thing to do is to import the sklearn package that has Logistic Regression, and then just create the respective object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogReg=LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a model, but it is \"empty\", so let's create the training and test sets now. \n",
    "\n",
    "We will create the model with 2/3 of the data (training set), and then 1/3 of the data is kept aside for later validation (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=int(len(f)*2.0/3)\n",
    "training=f[:split]\n",
    "test=f[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the x and y for the training and test set now. Notice that the y is the \"target\" variable, i.e. our \"stress\" column, and the x comes from **almost** all other columns. Let's check all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we need to create the x using EVERYTHING but the 'stress' variable, but also we need to remove \"pickups1\" (**why?**)\n",
    "\n",
    "Notice also that we have several dummy variables relating to the categorical 'time of day' variable. Recall that we discussed the method of dummy variables as a way of including categorical variables as predictors in our model.  In this case, the time of day variable has seven possible values and we created seven dummy variables using the pd.get_dummies() function. In logistic regression when including dumming variables for a categorical variable that takes k values, it recommended to only include k-1 of the dummies and not all k of them. The reason for this is that these k dummy variables sum to 1 leading to multicollinearity (linear dependence), which will make it difficult to interpret the coefficients. Thus, we will arbitrarily exclude the dummy 'time_of_day_night'. It is also important to note that when interpreting the coefficients for these dummy variables you interpret them as measuring an 'effect' relative to the baseline category that you left out (in this case the remaining six dummy variables should be interpreted as measuring effects relative to the 'time_of_day_night'). FYI, in Pandas, we can automatically create k-1 dummies instead of k dummies using the option 'drop_first=False' in the pd.get_dummies() function.\n",
    "\n",
    "The other methods for classification that we will cover (e.g., SVM, decision trees) are less sensitive to multicollinearity and there you can include all the seven dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=training[['pickups17_lag1', 'pickups17_lag2', 'pickups1_lag1',\n",
    "       'pickups1_lag2', 'pickups21_lag1', 'pickups21_lag2', 'pickups28_lag1',\n",
    "       'pickups28_lag2',  'temp', 'prcp','fog', 'rain_drizzle', 'is_weekend', 'time_of_day_afternoon',\n",
    "       'time_of_day_afternoon rush', 'time_of_day_evening',\n",
    "       'time_of_day_lunch time', 'time_of_day_morning',\n",
    "       'time_of_day_morning rush']]\n",
    "x_test=test[['pickups17_lag1', 'pickups17_lag2', 'pickups1_lag1',\n",
    "       'pickups1_lag2', 'pickups21_lag1', 'pickups21_lag2', 'pickups28_lag1',\n",
    "       'pickups28_lag2',  'temp', 'prcp','fog', 'rain_drizzle', 'is_weekend', 'time_of_day_afternoon',\n",
    "       'time_of_day_afternoon rush', 'time_of_day_evening',\n",
    "       'time_of_day_lunch time', 'time_of_day_morning',\n",
    "       'time_of_day_morning rush']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you understand things, don't forget to ALWAYS play with the code here... For example, what's the content of the new lists x_train and x_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now the ys are trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=training['stress']\n",
    "\n",
    "y_test=test['stress']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you understood, do you want to see what's inside these two vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have our Xs and Ys! Ready to go... it's totally trivial with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You trained your first Logistic Regression model. What's its accuracy (**on the test set**)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to try it on the training set? What do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the values tend to be similar, then congrats, it's very likely that your model is not overfitting! :-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, accuracy is not everything in a classifier. Another VERY interesting concept is the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it properly, let's first obain the predictions of our model using the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=LogReg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compare the predictions with the observations, using the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, ypred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: pay attention to the rows and columns in the confusion matrix. The format that Sklearn uses is different from what we had in the slides - the rows represent actual (true) values and the columns represent predicted values.\n",
    "You can see the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "Alternatively, we can also plot the confusion matrix just to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = ConfusionMatrixDisplay(cm)\n",
    "display.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we mentioned that accuracy is not a good measure if the data set is imbalanced. In our problem and data set, is the response variable imbalanced? In general, a better measure to use is the f1 score. We can compute this easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test,ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last useful thing about Logistic Regression. It is a parametric model, so its parameters beta (its \"coefficients\") can actually mean something. Let's take a look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit confusing. Which coefficient correspond to which variable? Let's make it more interpretable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cname, val in zip(x_train.columns, LogReg.coef_.tolist()[0]):\n",
    "    print(\"%s=%.3f\"%(cname, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the results? Take a look at the signs. Do they make sense? Try to play with the stress_threshold above (instead of 90 percentile, you can try others...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the notebook, we will include the dummy that we left out earlier - 'time_of_day_night':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train=training[['pickups17_lag1', 'pickups17_lag2', 'pickups1_lag1',\n",
    "       'pickups1_lag2', 'pickups21_lag1', 'pickups21_lag2', 'pickups28_lag1',\n",
    "       'pickups28_lag2',  'temp', 'prcp','fog', 'rain_drizzle', 'is_weekend', 'time_of_day_afternoon',\n",
    "       'time_of_day_afternoon rush', 'time_of_day_evening',\n",
    "       'time_of_day_lunch time', 'time_of_day_morning',\n",
    "       'time_of_day_morning rush','time_of_day_night']]\n",
    "x_test=test[['pickups17_lag1', 'pickups17_lag2', 'pickups1_lag1',\n",
    "       'pickups1_lag2', 'pickups21_lag1', 'pickups21_lag2', 'pickups28_lag1',\n",
    "       'pickups28_lag2',  'temp', 'prcp','fog', 'rain_drizzle', 'is_weekend', 'time_of_day_afternoon',\n",
    "       'time_of_day_afternoon rush', 'time_of_day_evening',\n",
    "       'time_of_day_lunch time', 'time_of_day_morning',\n",
    "       'time_of_day_morning rush','time_of_day_night']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1, we did almost everything for you. But now, we'll just help you with the import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that SVM's rely on computing 'distances' between features. Hence, it is critical that your data is standardized. Can you standardize the data first? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check its accuracy and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check its confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this model better than the Logistic regression model? Look at both the f1 score and the confusion matrix. In this case, do you care more about false positives or false negatives?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
