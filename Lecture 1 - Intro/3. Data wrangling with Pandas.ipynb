{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Business Analytics\n",
    "\n",
    "## Lecture 1 - Pandas, statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll explore in more depth the possibilities provided by the Python Pandas module. To make things less confusing, we decided to use the same dataset as last week. In fact, you'll start with doing exactly the **same** exercises. Then, we have an additional set of exercises deal with categorical data.\n",
    "\n",
    "**This notebook was designed to be done (or, at least, started) at the classroom. It's possible that mid-way or so, you'll feel comfortable to complete it by yourself, but we designed it such that we will be around most of the time. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's start. Since we are going to work with Pandas, let's just import it, shall we (you'll eventually also need Numpy, so import that too)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some data. Please open the file \"pickups_zone_1_15min.csv\". This corresponds to the series of taxi-pickups in New York zone 1 (an area in the Manhattan island). \n",
    "\n",
    "You can use the method read_csv(file), which **returns** a DataFrame:\n",
    "\n",
    "> df=pd.read_csv(file)\n",
    "\n",
    "df is a variable that now has a Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so we just loaded the file, now let's look at its content (and keep remembering how more complicated it was in Numpy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be sure, let's check how many lines the file actually has (now it should be 262848, one line less than with Numpy, why?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 lines of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, remember that we need to make a single field with the datetime (instead of 3 separate ones...)!\n",
    "\n",
    "Pandas actually allows us to simplify that right when we load the file. So, let's do it again, but now take a look at the following Stackexchange thread: https://stackoverflow.com/questions/38509512/pandas-read-csv-with-date-in-2-columns\n",
    "\n",
    "How about applying this here?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may happen that the (new) temporal field is still a string and not a datetime object. Can you correct that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, it can be useful to assign the time as index for our dataframe. Let's do that with the method set_index() of the DataFrame object. Notice that it doesn't do \"in-place\" changes, in other words, this command would not work: \n",
    "> df.set_index(KEY)\n",
    "\n",
    "You'd have to do\n",
    "\n",
    ">df=df.set_index(KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: You can always get a vector with the indeces themselves with \n",
    "> df.index\n",
    "\n",
    "To understand this, just try it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, just for fun (and to compare with the amount of work we did with Numpy), let's get **only** the pickups part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trivial, isn't it? It's even indexed correctly! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for our histogram for the pickups. If you can't remember or don't know how it's done, feel free to search online (something like \"Pandas histogram\")\n",
    "\n",
    "again, don't forget to add\n",
    "\n",
    ">import matplotlib.pyplot as plt\n",
    ">\n",
    ">%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's even scary how easy it is, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the scatter plot? It won't be as direct, you will have to use the pyplot one (the one you used last time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do that version with the colors. As last time, you need to create a new vector with the minutes since midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it may sound redundant, it may be useful to add this new series to the DataFrame (as a new column, in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's do the graph with the colors. \n",
    "\n",
    "Remember, imagine that the number of minutes since midnight (that you just created) corresponds to a color. The function scatter allows you to give this list straight away and plot it (just use the argument c, for example \"c=my_minute_since_midnight_list\". \n",
    "\n",
    "Do you want to try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the 24-hr average plot, where the x axis is 0 to 1440 (1440=24 hours X 60 minutes), and you show the average per minute.\n",
    "\n",
    "(don't forget to add the 5 and 95 quantiles) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small tip (mean of 15 minutes of the day):\n",
    "\n",
    "> df.loc[df['minute_of_day']==15].mean()\n",
    ">\n",
    ">\n",
    "Output:\n",
    "\n",
    "pickups          205.344047\n",
    "\n",
    "minute_of_day     15.000000\n",
    "\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "**IMPORTANT:** notice that the output above is not just the mean of the _pickups_ (which is what we want). It also returns the mean of the minute_of\\_day itself (which is obviously 15!)... This may confuse things later on, so how do you get the mean of pickups only?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this to do the 24-hr average plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, again, you did a lot of stuff. Why not reorganize it again?\n",
    "\n",
    "For example, a new function that receives the file name, and generates all the graphs you did so far.\n",
    "\n",
    "Wouldn't it be cool to just generate those by a single command?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very important task in Data Science modeling is to find (and understand) correlations between different variables. Let's do a few simple exercises.\n",
    "\n",
    "Let's start with a simple question: are the different areas correlated between them? If yes, it may be interesting knowledge. For example, maybe we can share data between them to predict better, later.\n",
    "\n",
    "\n",
    "Tip: Try to make a single DataFrame with ALL time series (s1, s17, s21, s28). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_read_csv(file):\n",
    "    df = pd.read_csv(file, sep=',', parse_dates={'dt': [0, 1, 2]})\n",
    "    df['datetime']=df['dt'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H %M\"))\n",
    "    del df['dt']\n",
    "    return df.set_index('datetime')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=my_read_csv(\"pickups_zone_1_15min.csv\")\n",
    "s17=my_read_csv(\"pickups_zone_17_15min.csv\")\n",
    "s21=my_read_csv(\"pickups_zone_21_15min.csv\")\n",
    "s28=my_read_csv(\"pickups_zone_28_15min.csv\")\n",
    "\n",
    "smerge=s1.join(s17, rsuffix=\"17\")\n",
    "smerge=smerge.join(s21, rsuffix=\"21\")\n",
    "smerge=smerge.join(s28, rsuffix=\"28\")\n",
    "smerge['pickups1']=smerge['pickups']\n",
    "del smerge['pickups']\n",
    "smerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smerge.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems all but area 28 are well correlated with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a more interesting question: are there correlations between a given area, and the other areas in earlier time steps? \n",
    "\n",
    "This is a VERY important one. If you find high correlation, for example, between area 1 at time t, with area 17 at time t-1, then you can use area 17 to predict area 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this, you need to play a little bit with the vectors. Let's call a vector that is shifted in time for 1 time step, a \"lag1\" vector. \n",
    "\n",
    "The process is similar to Numpy's, but there's a few tweaks. To make things simpler, we copy a solution from Stackoverflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLaggedFeatures(s,columns, lag=2,dropna=True):\n",
    "    '''\n",
    "    From http://stackoverflow.com/questions/20410312/how-to-create-a-lagged-data-structure-using-pandas-dataframe\n",
    "    Builds a new DataFrame to facilitate regressing over all possible lagged features\n",
    "    '''\n",
    "    if type(s) is pd.DataFrame:\n",
    "        new_dict={}\n",
    "        for c in s.columns:\n",
    "            new_dict[c]=s[c]\n",
    "        for col_name in columns:\n",
    "            new_dict[col_name]=s[col_name]\n",
    "            # create lagged Series\n",
    "            for l in range(1,lag+1):\n",
    "                new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n",
    "        res=pd.DataFrame(new_dict,index=s.index)\n",
    "\n",
    "    elif type(s) is pd.Series:\n",
    "        the_range=range(lag+1)\n",
    "        res=pd.concat([s.shift(i) for i in the_range],axis=1)\n",
    "        res.columns=['lag_%d' %i for i in the_range]\n",
    "    else:\n",
    "        print('Only works for DataFrame or Series')\n",
    "        return None\n",
    "    if dropna:\n",
    "        return res.dropna()\n",
    "    else:\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check those correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! Very interesting!! This means that you can use data from these other areas to predict for area 1... This is useful when there is missing data in area 1, for example... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get to the autocorrelogram. Well, in Pandas, this is trivial!\n",
    "\n",
    "You can use the auto_corr method:\n",
    "\n",
    ">s1=df['pickups'] #s1 is now a Series\n",
    ">\n",
    ">print(s1.autocorr(1))  #gives the autocorrelation of lag 1\n",
    "\n",
    "Output:\n",
    "\n",
    "0.97670270955196936\n",
    "\n",
    "\n",
    "Cool, now you just need to get values for the different lags (1, 2, 3, 4...) and plot the values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables, combining datasets\n",
    "\n",
    "Ok, you're almost ready to start rockin' with actual Data Science learning (next week: Regression models :-) ). \n",
    "\n",
    "There's a last thing before that. Quite often, the data that you get is not in numerical form. Two obvious examples are time partitioning, like weekdays (\"Monday\", \"Tuesday\"...) or time of day (\"Morning rush hour\", \"lunch time\"). These are in fact words, how can we use them in our modeling, if in practice it always requires numerical quantities?\n",
    "\n",
    "Particularly with mobility data, the examples above are quite common. For example, the time of day (rush hour VS low demand times) can be crucial for prediction!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a new column in our dataframe with the day of week (we'll do it for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smerge['weekday']=[el.weekday() for el in smerge.index]     #for each element in the index use the \"weekday\" function \n",
    "                                                  #(remember that the index is the datetime series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that it worked just take a look at the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, these are already numbers, but wait!... Do those quantities mean actually _something_ or aren't they just individual symbols for the week days?\n",
    "\n",
    "Other ways to respond to this question: if instead of these numbers you used others (e.g. starting at 1 instead of 0?) wouldn't the result be the same? If you use algebra on it, does the result make sense (Thursday-Tuesday=Wednesday...). \n",
    "\n",
    "The bottom line is that, in general, you shouldn't use these numbers directly in a regression model because week days do not correspond to quantities. This is a **categorical** variable, which can have one of a finite set of values. In our case:\n",
    "\n",
    "_weekday_ $\\in $ {Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's convert those values into something useful. A first obvious thing to do is to get whether a day is a week day or weekend. Let's define a simple function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(weekday):\n",
    "    return weekday in [5,6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test it...\n",
    "\n",
    "Another nice thing will be to actually put names there. Here's another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weekday_word(weekday):\n",
    "    if weekday==0:\n",
    "        return \"Monday\"\n",
    "    elif weekday==1:\n",
    "        return \"Tuesday\"\n",
    "    elif weekday==2:\n",
    "        return \"Wednesday\"\n",
    "    elif weekday==3:\n",
    "        return \"Thursday\"\n",
    "    elif weekday==4:\n",
    "        return \"Friday\"\n",
    "    elif weekday==5:\n",
    "        return \"Saturday\"\n",
    "    else:\n",
    "        return \"Sunday\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the new columns in our DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_dataframe['is_weekend']=[is_weekend(el) for el in your_dataframe['weekday']]\n",
    "your_dataframe['wd']=[weekday_word(el.weekday()) for el in your_dataframe.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the new DataFrame. Seems more useful, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the \"time of day\" information. Again, we provide you with the corresponding function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_day(hour):\n",
    "    if hour<7:\n",
    "        return \"night\"\n",
    "    elif hour<9:\n",
    "        return \"morning rush\"\n",
    "    elif hour<12:\n",
    "        return \"morning\"\n",
    "    elif hour<14:\n",
    "        return \"lunch time\"\n",
    "    elif hour<17:\n",
    "        return \"afternoon\"\n",
    "    elif hour<20:\n",
    "        return \"afternoon rush\"\n",
    "    elif hour<23:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "    \n",
    "#In fact, we created another function that has the SAME functionality as above just to show you how compact and elegant \n",
    "# Python can be. \n",
    "# Don't worry, you're not expected to reach this level before of a lot of experience. \n",
    "def time_of_day2(hour):    \n",
    "    h_thresholds=[7,8,12,14,17,20,23]\n",
    "    h_names=[\"night\", \"morning rush\", \"morning\", \"lunch time\", \"afternoon\", \"afternoon rush\", \"evening\", \"night\"]\n",
    "    return h_names[next(x[0] for x in enumerate(h_thresholds) if x[1] >hour)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you create the new column with time of day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other **very important** technique is to transform categorical variables into \"dummy variables\". A dummy variable is typically binary (so it's either 1/0, or True/False), and corresponds to one single possible categorical value. \n",
    "\n",
    "**So, you transform a single variable with N different values **\n",
    "\n",
    "_time of day -> {night, morning rush, morning...evening}_\n",
    "\n",
    "**into N dummy variables, each one with 2 values (1 or 0):**\n",
    "\n",
    "_night -> {0, 1}_\n",
    "\n",
    "_morning rush -> {0, 1}_\n",
    "\n",
    "_..._\n",
    "\n",
    "_evening -> {0, 1}_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we know it sounds like hard work, but in Pandas, it's all done trivially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_dummies=pd.get_dummies(your_dataframe, columns=['time_of_day'])  #this creates dummies for the \"time_of_day\" variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a well deserved reward for you. Just run the method describe() in your DataFrame and enjoy!... \n",
    "\n",
    ">s.describe()\n",
    ">\n",
    "\n",
    "Output:\n",
    "\n",
    "_something useful_  :-)\n",
    "\n",
    "\n",
    "Think about how many lines you would have needed in Numpy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
